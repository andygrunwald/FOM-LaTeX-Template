
@book{bintiamit.2016,
  title = {Analysis of Satellite Images for Disaster Detection},
  author = {Binti Amit, Siti Nor Khuzaimah and Shiraishi, Soma and Inoshita, Tetsuo and Aoki, Yoshimitsu},
  date = {2016-07-01},
  pages = {5192},
  doi = {10.1109/IGARSS.2016.7730352},
  pagetotal = {5189}
}

@article{cheng.2016,
  title = {A Survey on Object Detection in Optical Remote Sensing Images},
  author = {Cheng, Gong and Han, Junwei},
  date = {2016-07},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {117},
  pages = {11--28},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2016.03.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271616300144},
  urldate = {2021-10-13},
  abstract = {Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey 1) template matching-based object detection methods, 2) knowledge-based object detection methods, 3) object-based image analysis (OBIA)-based object detection methods, 4) machine learning-based object detection methods, and 5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/92RBNK8U/Cheng und Han - 2016 - A survey on object detection in optical remote sen.pdf}
}

@book{ditzinger.2013,
  title = {Illusionen des Sehens},
  author = {Ditzinger, Thomas},
  date = {2013},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37712-9},
  url = {http://link.springer.com/10.1007/978-3-642-37712-9},
  urldate = {2021-10-16},
  isbn = {978-3-642-37711-2 978-3-642-37712-9},
  langid = {german}
}

@article{govender.2007,
  title = {A Review of Hyperspectral Remote Sensing and Its Application in Vegetation and Water Resource Studies},
  author = {Govender, M. and Chetty, K. and Bulcock, H.},
  date = {2007},
  journaltitle = {Water SA},
  volume = {33},
  number = {2},
  issn = {0378-4738},
  doi = {10.4314/wsa.v33i2.49049},
  url = {https://www.ajol.info/index.php/wsa/article/view/49049},
  urldate = {2021-10-16},
  abstract = {Multispectral imagery has been used as the data source for water and land observational remote sensing from airborne and satellite systems since the early 1960s. Over the past two decades, advances in sensor technology have made it possible for the collection of several hundred spectral bands. This is commonly referred to as hyperspectral imagery. This review details the differences between multispectral and hyperspectral data; spatial and spectral resolutions and focuses on the application of hyperspectral imagery in water resource studies and, in particular the classification and mapping of land uses and vegetation.},
  issue = {2},
  langid = {english},
  keywords = {hyperspectral,multispectral,spatial resolution,spectral resolution,vegetation classification,water resources},
  file = {/Users/arturgergert/Zotero/storage/SEU3E595/Govender et al. - 2007 - A review of hyperspectral remote sensing and its a.pdf}
}

@article{guo.2018,
  title = {Geospatial {{Object Detection}} in {{High Resolution Satellite Images Based}} on {{Multi}}-{{Scale Convolutional Neural Network}}},
  author = {Guo, Wei and Yang, Wen and Zhang, Haijian and Hua, Guang},
  date = {2018-01-18},
  journaltitle = {Remote Sensing},
  volume = {10},
  number = {1},
  pages = {131},
  issn = {2072-4292},
  doi = {10.3390/rs10010131},
  url = {http://www.mdpi.com/2072-4292/10/1/131},
  urldate = {2021-10-10},
  abstract = {Daily acquisition of large amounts of aerial and satellite images has facilitated subsequent automatic interpretations of these images. One such interpretation is object detection. Despite the great progress made in this domain, the detection of multi-scale objects, especially small objects in high resolution satellite (HRS) images, has not been adequately explored. As a result, the detection performance turns out to be poor. To address this problem, we first propose a unified multi-scale convolutional neural network (CNN) for geospatial object detection in HRS images. It consists of a multi-scale object proposal network and a multi-scale object detection network, both of which share a multi-scale base network. The base network can produce feature maps with different receptive fields to be responsible for objects with different scales. Then, we use the multi-scale object proposal network to generate high quality object proposals from the feature maps. Finally, we use these object proposals with the multi-scale object detection network to train a good object detector. Comprehensive evaluations on a publicly available remote sensing object detection dataset and comparisons with several state-of-the-art approaches demonstrate the effectiveness of the presented method. The proposed method achieves the best mean average precision (mAP) value of 89.6\%, runs at 10 frames per second (FPS) on a GTX 1080Ti GPU.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/UCS46DDL/Guo et al. - 2018 - Geospatial Object Detection in High Resolution Sat.pdf}
}

@article{ibraheem.2015,
  title = {Early Detection of Melanoma Using Multispectral Imaging and Artificial Intelligence Techniques},
  author = {Ibraheem, Issa},
  date = {2015-06-01},
  journaltitle = {American Journal of Biomedical and Life Sciences. Special Issue: Spectral Imaging for Medical Diagnosis "Modern Tool for Molecular Imaging"},
  volume = {Vol. 3},
  pages = {29--33},
  doi = {10.11648/j.ajbls.s.2015030203.16},
  abstract = {Biomedical spectral imaging is a non-invasive, non-destructive method, and has an important role in melanoma detection and all skin lesions monitoring during their various stages. In addition to spatial information, it contains spectral information that describes structure such as melanin content, and melanoma thickness, which, very well improve the sensitivity and specificity of melanoma detection. This article aims to describe the design of a multispectral imaging system that utilizes Artificial Neural Networks and Genetic Algorithm (Artificial Intelligence) for spectral images classification, in order to reduce the processing time of spectral images, memory and cost of the system. All system (Hardware and Software) works as an automatic detection system for malignant melanoma, which identifies malignant melanoma and common (benign) nevi by using wavelength scanning method with; CCD camera, filters wheel, and only eight optical filters range from 430nm to 620nm. 47 study cases were imaged. Good results were obtained: the sensitivity 91.67\% and the specificity 91.43\%.},
  file = {/Users/arturgergert/Zotero/storage/DCRPQJVR/Ibraheem - 2015 - Early detection of melanoma using multispectral im.pdf}
}

@inproceedings{im.2019a,
  title = {Analysis and {{Optimization}} of {{CNN}}-Based {{Semantic Segmentation}} of {{Satellite Images}}},
  booktitle = {2019 {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  author = {Im, Heeji and Yang, Hoeseok},
  date = {2019-10},
  pages = {218--220},
  issn = {2162-1233},
  doi = {10.1109/ICTC46691.2019.8939782},
  abstract = {In this paper, we analyze and optimize the CNN (Convolutional Neural Network) based semantic segmentation network, U-Net. While U-Net is known to be the state-of-the-art in semantic segmentation, it is difficult to apply U-Net directly to the resource-constrained systems due to its high memory usage and computation requirement. To overcome this disadvantage, we apply the filter pruning method to optimize the network. Experimental results show that the memory usage is reduced by 0.26 times and the inference speed is enhanced by 0.57 times with 75\% filter pruning. In addition, the IOU (Intersection Over Union) and F1-score which reflect the accuracy of semantic segmentation, were only reduced by 4.7\% and 4.5\% respectively compared to the original U-net.},
  eventtitle = {2019 {{International Conference}} on {{Information}} and {{Communication Technology Convergence}} ({{ICTC}})},
  keywords = {DeepGlobe Challenge,Feature extraction,Image segmentation,Kernel,neural network,Optimization,pruning,Roads,satellite images,Satellites,semantic segmentation,Semantics},
  file = {/Users/arturgergert/Zotero/storage/Y2LA58IH/Im und Yang - 2019 - Analysis and Optimization of CNN-based Semantic Se.pdf;/Users/arturgergert/Zotero/storage/P7KG8LYH/8939782.html}
}

@article{landgrebe.1997,
  title = {On {{Information Extraction Principles}} for {{Hyperspectral Data}}},
  author = {Landgrebe, David},
  date = {1997},
  file = {/Users/arturgergert/Zotero/storage/HSYHDSYE/Landgrebe - 1997 - On Information Extraction Principles for Hyperspec.pdf}
}

@article{lecun.1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/arturgergert/Zotero/storage/I5CB948V/726791.html}
}

@online{long.2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date = {2015-03-08},
  eprint = {1411.4038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1411.4038},
  urldate = {2021-10-14},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/arturgergert/Zotero/storage/5RIR7ASK/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;/Users/arturgergert/Zotero/storage/X8IVDCYT/1411.html}
}

@article{mohanty.2020,
  title = {Deep {{Learning}} for {{Understanding Satellite Imagery}}: An {{Experimental Survey}}},
  shorttitle = {Deep {{Learning}} for {{Understanding Satellite Imagery}}},
  author = {Mohanty, Sharada Prasanna and Czakon, Jakub and Kaczmarek, Kamil A. and Pyskir, Andrzej and Tarasiewicz, Piotr and Kunwar, Saket and Rohrbach, Janick and Luo, Dave and Prasad, Manjunath and Fleer, Sascha and Göpfert, Jan Philip and Tandon, Akshat and Mollard, Guillaume and Rayaprolu, Nikhil and Salathe, Marcel and Schilling, Malte},
  date = {2020},
  journaltitle = {Frontiers in Artificial Intelligence},
  volume = {3},
  pages = {85},
  issn = {2624-8212},
  doi = {10.3389/frai.2020.534696},
  url = {https://www.frontiersin.org/article/10.3389/frai.2020.534696},
  urldate = {2021-10-13},
  abstract = {Translating satellite imagery into maps requires intensive effort and time, especially leading to inaccurate maps of the affected regions during disaster and conflict. The combination of availability of recent datasets and advances in computer vision made through deep learning paved the way toward automated satellite image translation. To facilitate research in this direction, we introduce the Satellite Imagery Competition using a modified SpaceNet dataset. Participants had to come up with different segmentation models to detect positions of buildings on satellite images. In this work, we present five approaches based on improvements of U-Net and Mask R-Convolutional Neuronal Networks models, coupled with unique training adaptations using boosting algorithms, morphological filter, Conditional Random Fields and custom losses. The good results—as high as AP=0.937 and AR=0.959—from these models demonstrate the feasibility of Deep Learning in automated satellite image annotation.},
  file = {/Users/arturgergert/Zotero/storage/CDP62UF3/Mohanty et al. - 2020 - Deep Learning for Understanding Satellite Imagery.pdf}
}

@article{piramanayagam.2018,
  title = {Supervised {{Classification}} of {{Multisensor Remotely Sensed Images Using}} a {{Deep Learning Framework}}},
  author = {Piramanayagam, Sankaranarayanan and Saber, Eli and Schwartzkopf, Wade and Koehler, Frederick},
  date = {2018-09-07},
  journaltitle = {Remote Sensing},
  volume = {10},
  pages = {1429},
  doi = {10.3390/rs10091429},
  abstract = {In this paper, we present a convolutional neural network (CNN)-based method to efficiently combine information from multisensor remotely sensed images for pixel-wise semantic classification. The CNN features obtained from multiple spectral bands are fused at the initial layers of deep neural networks as opposed to final layers. The early fusion architecture has fewer parameters and thereby reduces the computational time and GPU memory during training and inference. We also propose a composite fusion architecture that fuses features throughout the network. The methods were validated on four different datasets: ISPRS Potsdam, Vaihingen, IEEE Zeebruges and Sentinel-1, Sentinel-2 dataset. For the Sentinel-1,-2 datasets, we obtain the ground truth labels for three classes from OpenStreetMap. Results on all the images show early fusion, specifically after layer three of the network, achieves results similar to or better than a decision level fusion mechanism. The performance of the proposed architecture is also on par with the state-of-the-art results.},
  file = {/Users/arturgergert/Zotero/storage/HVH8JIIQ/Piramanayagam et al. - 2018 - Supervised Classification of Multisensor Remotely .pdf}
}

@online{pritt.2020,
  title = {Satellite {{Image Classification}} with {{Deep Learning}}},
  author = {Pritt, Mark and Chern, Gary},
  date = {2020-10-13},
  eprint = {2010.06497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.06497},
  urldate = {2021-10-13},
  abstract = {Satellite imagery is important for many applications including disaster response, law enforcement, and environmental monitoring. These applications require the manual identification of objects and facilities in the imagery. Because the geographic expanses to be covered are great and the analysts available to conduct the searches are few, automation is required. Yet traditional object detection and classification algorithms are too inaccurate and unreliable to solve the problem. Deep learning is a family of machine learning algorithms that have shown promise for the automation of such tasks. It has achieved success in image understanding by means of convolutional neural networks. In this paper we apply them to the problem of object and facility recognition in high-resolution, multi-spectral satellite imagery. We describe a deep learning system for classifying objects and facilities from the IARPA Functional Map of the World (fMoW) dataset into 63 different classes. The system consists of an ensemble of convolutional neural networks and additional neural networks that integrate satellite metadata with image features. It is implemented in Python using the Keras and TensorFlow deep learning libraries and runs on a Linux server with an NVIDIA Titan X graphics card. At the time of writing the system is in 2nd place in the fMoW TopCoder competition. Its total accuracy is 83\%, the F1 score is 0.797, and it classifies 15 of the classes with accuracies of 95\% or better.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/arturgergert/Zotero/storage/YXQMYQ56/Pritt und Chern - 2020 - Satellite Image Classification with Deep Learning.pdf;/Users/arturgergert/Zotero/storage/8MU7ALMK/2010.html}
}

@incollection{tarantino.2012,
  title = {8-{{Band Image Data Processing}} of the {{Worldview}}-2 {{Satellite}} in a {{Wide Area}} of {{Applications}}},
  author = {Tarantino, C. and Adamo, Maria and Pasquariello, Guido and Lovergine, Francesco and Blonda, Palma and Tomaselli, Valeria},
  date = {2012-01-27},
  doi = {10.5772/27499},
  isbn = {978-953-307-973-8},
  file = {/Users/arturgergert/Zotero/storage/P5XU2BFX/Tarantino et al. - 2012 - 8-Band Image Data Processing of the Worldview-2 Sa.pdf}
}

@article{upadhyay.2012,
  title = {Introduction {{To Satellite Imaging Technology And Creating Images Using Raw Data Obtained From Landsat Satellite}}},
  author = {Upadhyay, Pragati and Gupta, Sudha},
  date = {2012-10-12},
  volume = {1},
  pages = {41--45},
  abstract = {This paper aims at presenting various satellite imaging techniques. The paper also includes satellite sensors, their resolution, the format in which they store data, the bands which they use and their applications in satellite imaging technology. The concept of color composite images has also been introduced. Through this paper we intend to present a logic for creating images using raw data obtained from landsat satellite. For this context we have taken an example of KILAUEA VOLCANO, HAWAII image, taken on 23 MAY 2001 by Landsat-7 satellite. After looking at the output some important conclusions have been made. Later, few limitations associated with remote sensing have been listed.},
  file = {/Users/arturgergert/Zotero/storage/DLBQAW93/Upadhyay und Gupta - 2012 - Introduction To Satellite Imaging Technology And C.pdf}
}

@article{xueyunchen.2014,
  title = {Vehicle {{Detection}} in {{Satellite Images}} by {{Hybrid Deep Convolutional Neural Networks}}},
  author = {{Xueyun Chen} and {Shiming Xiang} and {Cheng-Lin Liu} and {Chun-Hong Pan}},
  date = {2014-10},
  journaltitle = {IEEE Geosci. Remote Sensing Lett.},
  volume = {11},
  number = {10},
  pages = {1797--1801},
  issn = {1545-598X, 1558-0571},
  doi = {10.1109/LGRS.2014.2309695},
  url = {http://ieeexplore.ieee.org/document/6778050/},
  urldate = {2021-10-10},
  abstract = {Detecting small objects such as vehicles in satellite images is a difficult problem. Many features (such as histogram of oriented gradient, local binary pattern, scale-invariant feature transform, etc.) have been used to improve the performance of object detection, but mostly in simple environments such as those on roads. Kembhavi et al. proposed that no satisfactory accuracy has been achieved in complex environments such as the City of San Francisco. Deep convolutional neural networks (DNNs) can learn rich features from the training data automatically and has achieved state-of-the-art performance in many image classification databases. Though the DNN has shown robustness to distortion, it only extracts features of the same scale, and hence is insufficient to tolerate large-scale variance of object. In this letter, we present a hybrid DNN (HDNN), by dividing the maps of the last convolutional layer and the maxpooling layer of DNN into multiple blocks of variable receptive field sizes or max-pooling field sizes, to enable the HDNN to extract variable-scale features. Comparative experimental results indicate that our proposed HDNN significantly outperforms the traditional DNN on vehicle detection.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/WP4H9XQJ/Xueyun Chen et al. - 2014 - Vehicle Detection in Satellite Images by Hybrid De.pdf}
}




@software{.2021,
  title = {Ncbi-Nlp/{{BioWordVec}}},
  date = {2021-07-09T07:07:07Z},
  origdate = {2019-02-22T16:32:34Z},
  url = {https://github.com/ncbi-nlp/BioWordVec},
  urldate = {2021-07-21},
  organization = {{NLM/NCBI BioNLP Research Group (PI: Zhiyong Lu)}}
}

@article{alshammari.2020,
  title = {The Impact of Using Different Annotation Schemes on Named Entity Recognition},
  author = {Alshammari, Nasser and Alanazi, Saad},
  date = {2020-11},
  journaltitle = {Egyptian Informatics Journal},
  pages = {S1110866520301596},
  issn = {11108665},
  doi = {10.1016/j.eij.2020.10.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1110866520301596},
  urldate = {2021-07-15},
  abstract = {Named entity recognition (NER) is a subfield of information extraction, which aims to detect and classify predefined named entities (e.g., people, locations, organizations, etc.) in a body of text. In the literature, many researchers have studied the application of different machine learning models and features to NER. However, few research efforts have been devoted to studying annotation schemes used to label multitoken named entities. In this research, we studied seven annotation schemes (IO, IOB, IOE, IOBES, BI, IE, and BIES) and their impact on the task of NER using five different classifiers. Our experiment was conducted on an in–house dataset that consists of 27 medical Arabic articles with more than 62,000 tokens. The IO annotation scheme outperformed other schemes with an F-measure score of 84.44\%. The closest competitor is the BIES scheme, which scored 72.78\%. The rest of the schemes’ scores ranged from 60.38\% to 69.18\%. Although the IO scheme achieved the best results, comparing it to the other schemes is not reasonable because it cannot identify consecutive entities, which the other schemes can do. Therefore, we also investigated the ability of recognizing consecutive entities and provided an analysis of the running-time complexity.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/6ENUC6WD/Alshammari und Alanazi - 2020 - The impact of using different annotation schemes o.pdf}
}

@inproceedings{beltagy.2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  date = {2019-11},
  pages = {3615--3620},
  doi = {10.18653/v1/D19-1371},
  url = {https://www.aclweb.org/anthology/D19-1371},
  urldate = {2021-06-27},
  abstract = {Iz Beltagy, Kyle Lo, Arman Cohan. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.},
  eventtitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  langid = {american},
  file = {/Users/arturgergert/Zotero/storage/XC27KAFV/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf;/Users/arturgergert/Zotero/storage/TRZHX8TP/D19-1371.html}
}

@online{brack.2021,
  title = {Analysing the {{Requirements}} for an {{Open Research Knowledge Graph}}: {{Use Cases}}, {{Quality Requirements}} and {{Construction Strategies}}},
  shorttitle = {Analysing the {{Requirements}} for an {{Open Research Knowledge Graph}}},
  author = {Brack, Arthur and Hoppe, Anett and Stocker, Markus and Auer, Sören and Ewerth, Ralph},
  date = {2021-02-11},
  eprint = {2102.06021},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.06021},
  urldate = {2021-07-18},
  abstract = {Current science communication has a number of drawbacks and bottlenecks which have been subject of discussion lately: Among others, the rising number of published articles makes it nearly impossible to get a full overview of the state of the art in a certain field, or reproducibility is hampered by fixed-length, document-based publications which normally cannot cover all details of a research work. Recently, several initiatives have proposed knowledge graphs (KG) for organising scientific information as a solution to many of the current issues. The focus of these proposals is, however, usually restricted to very specific use cases. In this paper, we aim to transcend this limited perspective and present a comprehensive analysis of requirements for an Open Research Knowledge Graph (ORKG) by (a) collecting and reviewing daily core tasks of a scientist, (b) establishing their consequential requirements for a KG-based system, (c) identifying overlaps and specificities, and their coverage in current solutions. As a result, we map necessary and desirable requirements for successful KG-based science communication, derive implications, and outline possible solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Digital Libraries,Computer Science - Information Retrieval},
  file = {/Users/arturgergert/Zotero/storage/VC75B6FM/Brack et al. - 2021 - Analysing the Requirements for an Open Research Kn.pdf}
}

@article{buchkremer.2019,
  title = {The {{Application}} of {{Artificial Intelligence Technologies}} as a {{Substitute}} for {{Reading}} and to {{Support}} and {{Enhance}} the {{Authoring}} of {{Scientific Review Articles}}},
  author = {Buchkremer, Rudiger and Demund, Alexander and Ebener, Stefan and Gampfer, Fabian and Jagering, David and Jurgens, Andreas and Klenke, Sebastian and Krimpmann, Dominik and Schmank, Jasmin and Spiekermann, Markus and Wahlers, Michael and Wiepke, Markus},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {65263--65276},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2917719},
  url = {https://ieeexplore.ieee.org/document/8718286/},
  urldate = {2021-07-15},
  abstract = {To gain a comprehensive overview of new scientific findings with the enormous, ever-increasing amount of published information, we apply a new combinatorial approach that complements the process of reading scientific articles by supplementing artificial intelligence technologies. We present a combinatorial approach, which we illustrate in the form of a ‘‘double funnel of artificial intelligence.’’ Our approach suggests to largely increase the amount of data at the beginning of the data collection process and to subsequently clean and enrich the data set in order to gain much more knowledge at the end of the procedure compared to a ‘‘classical’’ literature review. We use natural language processing and text visualization techniques to uncover findings that are generally unbeknown to the human reader due to the inability to process very large amounts of text. By illustrating the individual steps using practical examples taken from use cases, we demonstrate the merits of our approach. With our methodology, we are able to reproduce findings from ‘‘regular’’ review papers; however, we discover additional and new findings in different fields, such as data science or medicine. We also point out the limitations of our approach. Finally, we make suggestions as to how the methodology could be further developed.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/UW7A2PPU/Buchkremer et al. - 2019 - The Application of Artificial Intelligence Technol.pdf}
}

@online{cer.2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  date = {2018-04-12},
  eprint = {1803.11175},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.11175},
  urldate = {2021-07-18},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/arturgergert/Zotero/storage/4XY2FHU8/Cer et al. - 2018 - Universal Sentence Encoder.pdf}
}

@article{cho.2019,
  title = {Biomedical Named Entity Recognition Using Deep Neural Networks with Contextual Information},
  author = {Cho, Hyejin and Lee, Hyunju},
  date = {2019-12},
  journaltitle = {BMC Bioinformatics},
  volume = {20},
  number = {1},
  pages = {735},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-3321-4},
  url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3321-4},
  urldate = {2021-08-01},
  abstract = {Background: In biomedical text mining, named entity recognition (NER) is an important task used to extract information from biomedical articles. Previously proposed methods for NER are dictionary- or rule-based methods and machine learning approaches. However, these traditional approaches are heavily reliant on large-scale dictionaries, target-specific rules, or well-constructed corpora. These methods to NER have been superseded by the deep learning-based approach that is independent of hand-crafted features. However, although such methods of NER employ additional conditional random fields (CRF) to capture important correlations between neighboring labels, they often do not incorporate all the contextual information from text into the deep learning layers. Results: We propose herein an NER system for biomedical entities by incorporating n-grams with bi-directional long short-term memory (BiLSTM) and CRF; this system is referred to as a contextual long short-term memory networks with CRF (CLSTM). We assess the CLSTM model on three corpora: the disease corpus of the National Center for Biotechnology Information (NCBI), the BioCreative II Gene Mention corpus (GM), and the BioCreative V Chemical Disease Relation corpus (CDR). Our framework was compared with several deep learning approaches, such as BiLSTM, BiLSTM with CRF, GRAM-CNN, and BERT. On the NCBI corpus, our model recorded an F-score of 85.68\% for the NER of diseases, showing an improvement of 1.50\% over previous methods. Moreover, although BERT used transfer learning by incorporating more than 2.5 billion words, our system showed similar performance with BERT with an F-scores of 81.44\% for gene NER on the GM corpus and a outperformed F-score of 86.44\% for the NER of chemicals and diseases on the CDR corpus. We conclude that our method significantly improves performance on biomedical NER tasks. Conclusion: The proposed approach is robust in recognizing biological entities in text.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/8ANHK9RN/Cho und Lee - 2019 - Biomedical named entity recognition using deep neu.pdf}
}

@article{fries.,
  title = {Trove: {{Ontology}}-Driven Weak Supervision for Medical Entity Classification},
  author = {Fries, Jason A and Steinberg, Ethan and Khattar, Saelig and Fleming, Scott L and Posada, Jose and Callahan, Alison and Shah, Nigam H},
  pages = {24},
  abstract = {Motivation: Recognizing named entities (NER) and their associated attributes like negation are core tasks in natural language processing. However, manually labeling data for entity tasks is time consuming and expensive, creating barriers to using machine learning in new medical applications. Weakly supervised learning, which automatically builds imperfect training sets from low cost, less accurate labeling rules, offers a potential solution. Medical ontologies are compelling sources for generating labels, however combining multiple ontologies without ground truth data creates challenges due to label noise introduced by conflicting entity definitions. Key questions remain on the extent to which weakly supervised entity classification can be automated using ontologies, or how much additional task-specific rule engineering is required for state-of-the-art performance. Also unclear is how pre-trained language models, such as BioBERT, improve the ability to generalize from imperfectly labeled data.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/UPC3LFZJ/Fries et al. - Trove Ontology-driven weak supervision for medica.pdf}
}

@inproceedings{gurusamy.2014,
  title = {Preprocessing {{Techniques}} for {{Text Mining}}},
  author = {Gurusamy, Vairaprakash and Kannan, Subbu},
  date = {2014-10-09},
  abstract = {Preprocessing is an important task and critical step in Text mining, Natural Language Processing (NLP) and information retrieval (IR). In the area of Text Mining, data preprocessing used for extracting interesting and non-trivial and knowledge from unstructured text data. Information Retrieval (IR) is essentially a matter of deciding which documents in a collection should be retrieved to satisfy a user's need for information. The user's need for information is represented by a query or profile, and contains one or more search terms, plus some additional information such as weight of the words. Hence, the retrieval decision is made by comparing the terms of the query with the index terms (important words or phrases) appearing in the document itself. The decision may be binary (retrieve/reject), or it may involve estimating the degree of relevance that the document has to query. Unfortunately, the words that appear in documents and in queries often have many structural variants. So before the information retrieval from the documents, the data preprocessing techniques are applied on the target data set to reduce the size of the data set which will increase the effectiveness of IR System The objective of this study is to analyze the issues of preprocessing methods such as Tokenization, Stop word removal and Stemming for the text documents Keywords: Text Mining, NLP, IR, Stemming},
  file = {/Users/arturgergert/Zotero/storage/ZKBV5G7P/Gurusamy und Kannan - 2014 - Preprocessing Techniques for Text Mining.pdf}
}

@inproceedings{hakala.2016,
  title = {Syntactic Analyses and Named Entity Recognition for {{PubMed}} and {{PubMed Central}} — up-to-the-Minute},
  booktitle = {Proceedings of the 15th {{Workshop}} on {{Biomedical Natural Language Processing}}},
  author = {Hakala, Kai and Kaewphan, Suwisa and Salakoski, Tapio and Ginter, Filip},
  date = {2016-08},
  pages = {102--107},
  publisher = {{Association for Computational Linguistics}},
  location = {{Berlin, Germany}},
  doi = {10.18653/v1/W16-2913},
  url = {https://aclanthology.org/W16-2913},
  urldate = {2021-08-03},
  file = {/Users/arturgergert/Zotero/storage/ZGR62LF7/Hakala et al. - 2016 - Syntactic analyses and named entity recognition fo.pdf}
}

@article{harige.,
  title = {Generating a {{Large}}-{{Scale Entity Linking Dictionary}} from {{Wikipedia Link Structure}} and {{Article Text}}},
  author = {Harige, Ravindra and Buitelaar, Paul},
  pages = {4},
  abstract = {Wikipedia has been increasingly used as a knowledge base for open-domain Named Entity Linking and Disambiguation. In this task, a dictionary with entity surface forms plays an important role in finding a set of candidate entities for the mentions in text. Existing dictionaries mostly rely on the Wikipedia link structure, like anchor texts, redirect links and disambiguation links. In this paper, we introduce a dictionary for Entity Linking that includes name variations extracted from Wikipedia article text, in addition to name variations derived from the Wikipedia link structure. With this approach, we show an increase in the coverage of entities and their mentions in the dictionary in comparison to other Wikipedia based dictionaries.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/QXDLRT5A/Harige und Buitelaar - Generating a Large-Scale Entity Linking Dictionary.pdf}
}

@article{huang.,
  title = {Revised {{JNLPBA Corpus}}: {{A Revised Version}} of {{Biomedical NER Corpus}} for {{Relation Extraction Task}}},
  author = {Huang, Ming-Siang},
  pages = {17},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/ABU5D4QB/Huang - Revised JNLPBA Corpus A Revised Version of Biomed.pdf}
}

@article{jivani.2011,
  title = {A {{Comparative Study}} of {{Stemming Algorithms}}},
  author = {Jivani, Anjali},
  date = {2011-11-01},
  journaltitle = {Int. J. Comp. Tech. Appl.},
  volume = {2},
  pages = {1930--1938},
  abstract = {Stemming is a pre-processing step in Text Mining applications as well as a very common requirement of Natural Language processing functions. In fact it is very important in most of the Information Retrieval systems. The main purpose of stemming is to reduce different grammatical forms / word forms of a word like its noun, adjective, verb, adverb etc. to its root form. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have discussed different methods of stemming and their comparisons in terms of usage, advantages as well as limitations. The basic difference between stemming and lemmatization is also discussed},
  file = {/Users/arturgergert/Zotero/storage/HBPLRXDC/Jivani - 2011 - A Comparative Study of Stemming Algorithms.pdf}
}

@article{kalyan.2021,
  title = {{{AMMU}} – {{A Survey}} of {{Transformer}}-Based {{Biomedical Pretrained Language Models}}},
  author = {Kalyan, Katikapalli Subramanyam and Rajasekharan, Ajit and Sangeetha, Sivanesan},
  date = {2021},
  pages = {42},
  abstract = {Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing (NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success of these models in the general domain, the biomedical research community has developed various in-domain PLMs starting from BioBERT to the latest BioMegatron and CoderBERT models. We strongly believe there is a need for a survey paper that can provide a comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a brief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain. We introduce a taxonomy for transformerbased BPLMs and then discuss all the models. We discuss various challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research community to further improve transformer-based BPLMs.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/BV4GNAB4/Kalyan et al. - 2021 - AMMU – A Survey of Transformer-based Biomedical Pr.pdf}
}

@article{karadeniz.2019,
  title = {Linking Entities through an Ontology Using Word Embeddings and Syntactic Re-Ranking},
  author = {Karadeniz, İlknur and Özgür, Arzucan},
  date = {2019-12},
  journaltitle = {BMC Bioinformatics},
  volume = {20},
  number = {1},
  pages = {156},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-2678-8},
  url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2678-8},
  urldate = {2021-08-01},
  abstract = {Background: Although there is an enormous number of textual resources in the biomedical domain, currently, manually curated resources cover only a small part of the existing knowledge. The vast majority of these information is in unstructured form which contain nonstandard naming conventions. The task of named entity recognition, which is the identification of entity names from text, is not adequate without a standardization step. Linking each identified entity mention in text to an ontology/dictionary concept is an essential task to make sense of the identified entities. This paper presents an unsupervised approach for the linking of named entities to concepts in an ontology/dictionary. We propose an approach for the normalization of biomedical entities through an ontology/dictionary by using word embeddings to represent semantic spaces, and a syntactic parser to give higher weight to the most informative word in the named entity mentions. Results: We applied the proposed method to two different normalization tasks: the normalization of bacteria biotope entities through the Onto-Biotope ontology and the normalization of adverse drug reaction entities through the Medical Dictionary for Regulatory Activities (MedDRA). The proposed method achieved a precision score of 65.9\%, which is 2.9 percentage points above the state-of-the-art result on the BioNLP Shared Task 2016 Bacteria Biotope test data and a macro-averaged precision score of 68.7\% on the Text Analysis Conference 2017 Adverse Drug Reaction test data. Conclusions: The core contribution of this paper is a syntax-based way of combining the individual word vectors to form vectors for the named entity mentions and ontology concepts, which can then be used to measure the similarity between them. The proposed approach is unsupervised and does not require labeled data, making it easily applicable to different domains.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/YE4WBH97/Karadeniz und Özgür - 2019 - Linking entities through an ontology using word em.pdf}
}

@article{khyani.2021,
  title = {An {{Interpretation}} of {{Lemmatization}} and {{Stemming}} in {{Natural Language Processing}}},
  author = {Khyani, Divya and B S, Siddhartha},
  date = {2021-01-07},
  journaltitle = {Shanghai Ligong Daxue Xuebao/Journal of University of Shanghai for Science and Technology},
  volume = {22},
  pages = {350--357},
  abstract = {This research paper aims to provide a general perspective on Natural Language processing, lemmatization, and Stemming. It focuses on building up a base that helps in attaining a general idea over the technology. It explains the concept of Natural Language Processing, its evolution over the years, its applications, its merits, and demerits. In addition to that, it also gives a brief idea about concepts such as Lemmatization and Stemming. It helps in understanding their working, the algorithms that come under these processes, and their applications. At last, this research provides the comparison of lemmatization and stemming, attempting to find which one is the best.},
  file = {/Users/arturgergert/Zotero/storage/3AWFHC2X/Khyani und B S - 2021 - An Interpretation of Lemmatization and Stemming in.pdf}
}

@article{kim.2013,
  title = {Automatic Keyphrase Extraction from Scientific Articles},
  author = {Kim, Su Nam and Medelyan, Olena and Kan, Min-Yen and Baldwin, Timothy},
  date = {2013-09},
  journaltitle = {Lang Resources \& Evaluation},
  volume = {47},
  number = {3},
  pages = {723--742},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-012-9210-3},
  url = {http://link.springer.com/10.1007/s10579-012-9210-3},
  urldate = {2021-06-27},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/W96ULK4T/Baldwin - Kim, Su Nam, Olena Medelyan, Min-Yen Kan and Timot.pdf}
}

@article{kronberger.2000,
  title = {Keywords in Context: {{Statistical}} Analysis of Text Features},
  shorttitle = {Keywords in Context},
  author = {Kronberger, Nicole and Wagner, Wolfgang},
  date = {2000-01-01},
  journaltitle = {Qualitative Researching with Text, Image and Sound: A Practical Handbook},
  pages = {299--317},
  file = {/Users/arturgergert/Zotero/storage/V3CHTTVD/Kronberger und Wagner - 2000 - Keywords in context Statistical analysis of text .pdf}
}

@online{li.2020,
  title = {A {{Survey}} on {{Deep Learning}} for {{Named Entity Recognition}}},
  author = {Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  date = {2020-03-18},
  eprint = {1812.09449},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1812.09449},
  urldate = {2021-08-04},
  abstract = {Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/arturgergert/Zotero/storage/NX3XSRPY/Li et al. - 2020 - A Survey on Deep Learning for Named Entity Recogni.pdf;/Users/arturgergert/Zotero/storage/7KFSKCSI/1812.html}
}

@online{luo.2020,
  title = {Named {{Entity Recognition Only}} from {{Word Embeddings}}},
  author = {Luo, Ying and Zhao, Hai and Zhan, Junlang},
  date = {2020-10-05},
  eprint = {1909.00164},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.00164},
  urldate = {2021-07-21},
  abstract = {Deep neural network models have helped named entity (NE) recognition achieve amazing performance without handcrafting features. However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings. We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and refine these coarse-grained annotations through neural networks. Extensive experiments on CoNLL benchmark datasets demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/arturgergert/Zotero/storage/ALMIK9MG/Luo et al. - 2020 - Named Entity Recognition Only from Word Embeddings.pdf;/Users/arturgergert/Zotero/storage/ASJULA9B/1909.html}
}

@article{mohan.2015,
  title = {Preprocessing {{Techniques}} for {{Text Mining}} - {{An Overview}}},
  author = {Mohan, Vijayarani},
  date = {2015-02-27},
  abstract = {Data mining is used for finding the useful information from the large amount of data. Data mining techniques are used to implement and solve different types of research problems. The research related. It is also called knowledge discovery in text (KDT) or knowledge of intelligent text analysis. Text mining is a technique which extracts information from both structured and unstructured data and also finding patterns. Text mining techniques are used in various types of research domains like natural language processing, information retrieval, text classification and text clustering.},
  file = {/Users/arturgergert/Zotero/storage/76KLP9FT/Mohan - 2015 - Preprocessing Techniques for Text Mining - An Over.pdf}
}

@article{nadeau.2007,
  title = {A Survey of Named Entity Recognition and Classification},
  author = {Nadeau, David and Sekine, S.},
  date = {2007},
  doi = {10.1075/LI.30.1.03NAD},
  abstract = {This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.},
  file = {/Users/arturgergert/Zotero/storage/M7U26DBZ/Nadeau und Sekine - A survey of named entity recognition and classific.pdf}
}

@article{nasar.2018,
  title = {Information Extraction from Scientific Articles: A Survey},
  shorttitle = {Information Extraction from Scientific Articles},
  author = {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
  date = {2018-12},
  journaltitle = {Scientometrics},
  volume = {117},
  number = {3},
  pages = {1931--1990},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-018-2921-5},
  url = {http://link.springer.com/10.1007/s11192-018-2921-5},
  urldate = {2021-06-27},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/3Y5P7SLZ/Nasar et al. - Information extraction from scientific articles a.pdf}
}

@article{pahwa.2018,
  title = {Sentiment {{Analysis}}- {{Strategy}} for {{Text Pre}}-{{Processing}}},
  author = {Pahwa, Bhumika and Taruna, S. and Kasliwal, Neeti},
  date = {2018-04-17},
  journaltitle = {IJCA},
  volume = {180},
  number = {34},
  pages = {15--18},
  issn = {09758887},
  doi = {10.5120/ijca2018916865},
  url = {http://www.ijcaonline.org/archives/volume180/number34/pahwa-2018-ijca-916865.pdf},
  urldate = {2021-08-03},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/SX2K4AZJ/Pahwa et al. - 2018 - Sentiment Analysis- Strategy for Text Pre-Processi.pdf}
}

@article{savic.2020,
  title = {Chronic Urticaria in the Real‐life Clinical Practice Setting in the {{UK}}: Results from the Noninterventional Multicentre {{AWARE}} Study},
  shorttitle = {Chronic Urticaria in the Real‐life Clinical Practice Setting in the {{UK}}},
  author = {Savic, S. and Leeman, L. and El‐Shanawany, T. and Ellis, R. and Gach, J.E. and Marinho, S. and Wahie, S. and Sargur, R. and Bewley, A.P. and Nakonechna, A. and Randall, R. and Fragkas, N. and Somenzi, O. and Marsland, A.},
  date = {2020-12},
  journaltitle = {Clin. Exp. Dermatol.},
  volume = {45},
  number = {8},
  pages = {1003--1010},
  issn = {0307-6938, 1365-2230},
  doi = {10.1111/ced.14230},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/ced.14230},
  urldate = {2021-06-29},
  abstract = {Background. Chronic urticaria (CU) is a skin condition characterised by repeated occurrence of itchy wheals and/or angioedema for {$>$}6 weeks. Aim. To provide data demonstrating the real-life burden of CU in the UK. Methods. This UK subset of the worldwide, prospective, non-interventional AWARE study included patients aged 18–75 years diagnosed with H1-antihistamine (H1-AH)-refractory chronic spontaneous urticaria (CSU) for {$>$}2 months. Baseline characteristics, disease activity, treatments, comorbidities and healthcare resource use were documented. Quality of life, work productivity and activity impairment were assessed. Results. Baseline analysis included 252 UK patients. Mean age and body mass index were 45.0 years and 29.0 kg/m2, respectively. Most patients were female (77.8\%) and had moderate/severe disease activity (mean Urticaria Activity Score over 7 days, 18.4) and a ‘spontaneous’ component to their CU (73.4\% CSU; 24.6\% CSU and chronic inducible urticaria). Common comorbidities included depression/anxiety (24.6\%), asthma (23.8\%) and allergic rhinitis (12.7\%). A previous treatment was recorded for 57.9\% of patients. Mean Dermatology Life Quality Index score was 9.5 and patients reported impairments in work productivity and activity. Healthcare resource use was high. Severity of CSU was associated with gender, obesity, anxiety and diagnosis. Only 28.5\% of patients completed all nine study visits, limiting analysis of long-term treatment patterns and disease impact. Conclusions. Adult H1-AH-refractory CU patients in the UK reported high rates of healthcare resource use and impairment in quality of life, work productivity and activity at baseline. The differing structures of UK healthcare may explain the high study discontinuation rates versus other countries.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/VQCC9TM5/Savic et al. - 2020 - Chronic urticaria in the real‐life clinical practi.pdf}
}

@online{sevgili.2021,
  title = {Neural {{Entity Linking}}: {{A Survey}} of {{Models Based}} on {{Deep Learning}}},
  shorttitle = {Neural {{Entity Linking}}},
  author = {Sevgili, Ozge and Shelmanov, Artem and Arkhipov, Mikhail and Panchenko, Alexander and Biemann, Chris},
  date = {2021-01-29},
  eprint = {2006.00575},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.00575},
  urldate = {2021-08-04},
  abstract = {In this survey, we provide a comprehensive description of recent neural entity linking (EL) systems developed since 2015 as a result of the "deep learning revolution" in NLP. Our goal is to systemize design features of neural entity linking systems and compare their performances to the best classic methods on the common benchmarks. We distill generic architectural components of a neural EL system, like candidate generation and entity ranking summarizing the prominent methods for each of them, such as approaches to mention encoding based on the self-attention architecture. The vast variety of modifications of this general neural entity linking architecture are grouped by several common themes: joint entity recognition and linking, models for global linking, domain-independent techniques including zero-shot and distant supervision methods, and cross-lingual approaches. Since many neural models take advantage of pre-trained entity embeddings to improve their generalization capabilities, we provide an overview of popular entity embedding techniques. Finally, we briefly discuss applications of entity linking, focusing on the recently emerged use-case of enhancing deep pre-trained masked language models such as BERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/arturgergert/Zotero/storage/GNMZQ7A2/Sevgili et al. - 2021 - Neural Entity Linking A Survey of Models Based on.pdf;/Users/arturgergert/Zotero/storage/A4WTSHRF/2006.html}
}

@inproceedings{tjongkimsang.2003,
  title = {Introduction to the {{CoNLL}}-2003 {{Shared Task}}: {{Language}}-{{Independent Named Entity Recognition}}},
  shorttitle = {Introduction to the {{CoNLL}}-2003 {{Shared Task}}},
  booktitle = {Proceedings of the {{Seventh Conference}} on {{Natural Language Learning}} at {{HLT}}-{{NAACL}} 2003},
  author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
  date = {2003},
  pages = {142--147},
  url = {https://aclanthology.org/W03-0419},
  urldate = {2021-08-05},
  file = {/Users/arturgergert/Zotero/storage/C4M4MZFF/Tjong Kim Sang und De Meulder - 2003 - Introduction to the CoNLL-2003 Shared Task Langua.pdf}
}

@book{voutilainen.2012,
  title = {Part-of-{{Speech Tagging}}},
  author = {Voutilainen, Atro},
  editor = {Mitkov, Ruslan},
  date = {2012-09-18},
  volume = {1},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199276349.013.0011},
  url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-11},
  urldate = {2021-08-03},
  abstract = {This chapter outlines recently used methods for designing part-of-speech taggers, computer programs for assigning contextually appropriate grammatical descriptors to words in texts. First, the general architecture and task setting are described. A brief history of tagging follows, where some central approaches to tagging are described: (i) taggers based on handwritten local rules; (ii) taggers based on n-grams automatically derived from tagged text corpora; (iii) taggers based on hidden Markov models; (iv) taggers using automatically generated symbolic language models derived using methods from machine learning; (v) taggers based on handwritten global rules; and (vi) hybrid taggers, attempts to combine advantages of handwritten taggers and automatically generated taggers. Since statistical and machine-learning approaches are described in more detail in other chapters of this volume, while handwritten tagging rules are not, the remainder of this chapter describes the design of linguistic rule-based disambiguators in more detail.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/IKN96XKC/Voutilainen - 2012 - Part-of-Speech Tagging.pdf}
}

@report{wang.2020,
  type = {preprint},
  title = {{{NERO}}: {{A Biomedical Named}}-Entity ({{Recognition}}) {{Ontology}} with a {{Large}}, {{Annotated Corpus Reveals Meaningful Associations Through Text Embedding}}},
  shorttitle = {{{NERO}}},
  author = {Wang, Kanix and Stevens, Robert and Alachram, Halima and Li, Yu and Soldatova, Larisa and King, Ross and Ananiadou, Sophia and Li, Maolin and Christopoulou, Fenia and Ambite, Jose Luis and Garg, Sahil and Hermjakob, Ulf and Marcu, Daniel and Sheng, Emily and Beißbarth, Tim and Wingender, Edgar and Galstyan, Aram and Gao, Xin and Chambers, Brendan and Khomtchouk, Bohdan B. and Evans, James A. and Rzhetsky, Andrey},
  date = {2020-11-06},
  institution = {{Systems Biology}},
  doi = {10.1101/2020.11.05.368969},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.11.05.368969},
  urldate = {2021-08-01},
  abstract = {Machine reading is essential for unlocking valuable knowledge contained in the millions of existing biomedical documents. Over the last two decades             1,2             , the most dramatic advances in machine-reading have followed in the wake of critical corpus development             3             . Large, well-annotated corpora have been associated with punctuated advances in machine reading methodology and automated knowledge extraction systems in the same way that ImageNet             4             was fundamental for developing machine vision techniques. This study contributes six components to an advanced, named-entity analysis tool for biomedicine: (a) a new, Named-Entity Recognition Ontology (NERO) developed specifically for describing entities in biomedical texts, which accounts for diverse levels of ambiguity, bridging the scientific sublanguages of molecular biology, genetics, biochemistry, and medicine; (b) detailed guidelines for human experts annotating hundreds of named-entity classes; (c) pictographs for all named entities, to simplify the burden of annotation for curators; (d) an original, annotated corpus comprising 35,865 sentences, which encapsulate 190,679 named entities and 43,438 events connecting two or more entities; (e) validated, off-the-shelf, named-entity recognition automated extraction, and; (f) embedding models that demonstrate the promise of biomedical associations embedded within this corpus.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/WKK5QBT5/Wang et al. - 2020 - NERO A Biomedical Named-entity (Recognition) Onto.pdf}
}

@article{zhang.2013,
  title = {Unsupervised Biomedical Named Entity Recognition: {{Experiments}} with Clinical and Biological Texts},
  shorttitle = {Unsupervised Biomedical Named Entity Recognition},
  author = {Zhang, Shaodian and Elhadad, Noémie},
  date = {2013-12},
  journaltitle = {Journal of Biomedical Informatics},
  volume = {46},
  number = {6},
  pages = {1088--1098},
  issn = {15320464},
  doi = {10.1016/j.jbi.2013.08.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046413001196},
  urldate = {2021-08-01},
  abstract = {Named entity recognition is a crucial component of biomedical natural language processing, enabling information extraction and ultimately reasoning over and knowledge discovery from text. Much progress has been made in the design of rule-based and supervised tools, but they are often genre and task dependent. As such, adapting them to different genres of text or identifying new types of entities requires major effort in re-annotation or rule development. In this paper, we propose an unsupervised approach to extracting named entities from biomedical text. We describe a stepwise solution to tackle the challenges of entity boundary detection and entity type classification without relying on any handcrafted rules, heuristics, or annotated data. A noun phrase chunker followed by a filter based on inverse document frequency extracts candidate entities from free text. Classification of candidate entities into categories of interest is carried out by leveraging principles from distributional semantics. Experiments show that our system, especially the entity classification step, yields competitive results on two popular biomedical datasets of clinical notes and biological literature, and outperforms a baseline dictionary match approach. Detailed error analysis provides a road map for future work.},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/MBKQED79/Zhang und Elhadad - 2013 - Unsupervised biomedical named entity recognition .pdf}
}

@article{zhang.2019,
  title = {{{BioWordVec}}, Improving Biomedical Word Embeddings with Subword Information and {{MeSH}}},
  author = {Zhang, Yijia and Chen, Qingyu and Yang, Zhihao and Lin, Hongfei and Lu, Zhiyong},
  date = {2019-12},
  journaltitle = {Sci Data},
  volume = {6},
  number = {1},
  pages = {52},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0055-0},
  url = {http://www.nature.com/articles/s41597-019-0055-0},
  urldate = {2021-07-21},
  langid = {english},
  file = {/Users/arturgergert/Zotero/storage/S4M5KE5B/Zhang et al. - 2019 - BioWordVec, improving biomedical word embeddings w.pdf}
}


